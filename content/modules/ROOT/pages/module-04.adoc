= Model Serving

== Prerequisites for this model
- Your workbench is configured and running.
- You have a data connection
- The git repo is cloned 
For all steps see Module `Setup RHODS workbench`


== Download a pre-trained wvi model and upload it to S3

In case you have to not had the time or resources to train the model by yourself, you can download a pre-trained wvi model and upload it to your S3 bucket.

* Open your workbench (with your wvi data connection)
* Navigate to `windy-journey/ml/pytorch` and open `02_Visual_Inspection_Upload_Pretrained_Model.ipynb`
* Run the notebook to upload the model

== Configure RHODS model serving

* Create model server in your data science project
 ** Models and model servers \-> *`Configure server`*
 ** Server name: `+wvi-{your-name}+`
 ** Serving runtime: `OpenVINO Model Server`
 ** Number of model server replicas to deploy: `1`
 ** Model server size: `Small`
 ** Model route: \-> `Check/Enable` _'Make deployed models available through an external route'_
 ** Token authorization \-> `Uncheck/Disable` _'Require token authentication'_
 ** \-> *`Configure`*
* Deploy the trained model \-> *`Deploy Model`*
 ** Model Name: `+wvi-{your-name}+`
 ** Model framework: `onnx - 1`
 ** Model location: `Existing data connection`
 ** Name: `+wvi-{your-name}+`
 ** Folder path: `+wvi-{your-name}-best.onnx+`
 ** \-> *`Deploy`*
* Wait until Status is green / loaded
 ** Copy and save the inference URL


 == Test inferencing with a REST API call

Try out how an ML REST call could be integrated into your 'intelligent' Python application.

* Return to the workbench
* Navigate to `windy-journey/ml/pytorch` and open `03_Visual_Inspection_YOLOv5_Infer_Rest.ipynb`
* Explore and run cells step by step
 ** Please donÂ´t forget to update the inferencing URL